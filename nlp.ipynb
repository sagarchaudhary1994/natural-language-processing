{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph= 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "stemmer =PorterStemmer()\n",
    "print(stemmer.stem(\"run\"))\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('History', 'happy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lammetizer = WordNetLemmatizer()\n",
    "lammetizer.lemmatize(\"History\"),lammetizer.lemmatize(\"happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lammetizer = WordNetLemmatizer()\n",
    "corpus_stemmed = []\n",
    "corpus_lammetized = []\n",
    "## apply stemming and lammetization on each word of the paragraph\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    ##apply the stemming on each word.\n",
    "    word_stem = [stemmer.stem(word) for word in words]\n",
    "    word_lammetized = [lammetizer.lemmatize(word) for word in words]\n",
    "    corpus_stemmed.append(' '.join(word_stem))\n",
    "    corpus_lammetized.append(' '.join(word_lammetized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur languag process ( nlp ) is a subfield of linguist , comput scienc , and artifici intellig concern with the interact between comput and human languag , in particular how to program comput to process and analyz larg amount of natur languag data .', \"the goal is a comput capabl of `` understand '' the content of document , includ the contextu nuanc of the languag within them .\", 'the technolog can then accur extract inform and insight contain in the document as well as categor and organ the document themselv .']\n",
      "\n",
      "\n",
      " ['Natural language processing ( NLP ) is a subfield of linguistics , computer science , and artificial intelligence concerned with the interaction between computer and human language , in particular how to program computer to process and analyze large amount of natural language data .', \"The goal is a computer capable of `` understanding '' the content of document , including the contextual nuance of the language within them .\", 'The technology can then accurately extract information and insight contained in the document a well a categorize and organize the document themselves .']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_stemmed)\n",
    "print(\"\\n\\n\",corpus_lammetized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sagar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Number of stopwords in nltk- 179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stp_words = stopwords.words(\"english\")\n",
    "print(stp_words)\n",
    "print(f\"Number of stopwords in nltk- {len(stp_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changes\n",
      "change\n",
      "changer\n",
      "changed\n",
      "changing\n",
      "changeable\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "for i in [\"changes\",\"change\",\"changer\",\"changed\",\"changing\",\"changeable\",\"better\"]:\n",
    "    print(lammetizer.lemmatize(i,pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing ( NLP ) is a subfield of linguistics , computer science , and artificial intelligence concerned with the interaction between computer and human language , in particular how to program computer to process and analyze large amount of natural language data .',\n",
       " \"The goal is a computer capable of `` understanding '' the content of document , including the contextual nuance of the language within them .\",\n",
       " 'The technology can then accurately extract information and insight contained in the document a well a categorize and organize the document themselves .']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lammetized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create corpus by removing the stop words\n",
    "corpus_lammetized=[]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i].lower())\n",
    "    lammetized_words = [lammetizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    corpus_lammetized.append(' '.join(lammetized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing ( NLP ) subfield linguistics , computer science , artificial intelligence concerned interaction computer human language , particular program computer process analyze large amount natural language data .', \"The goal computer capable `` understanding '' content document , including contextual nuance language within .\", 'The technology accurately extract information insight contained document well categorize organize document .']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_lammetized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    text= re.sub('[^a-zA-Z1-9]',' ',sentences[i])\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [lammetizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    corpus.append(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp subfield linguistics computer science artificial intelligence concerned interaction computer human language particular program computer process analyze large amount natural language data',\n",
       " 'goal computer capable understanding content document including contextual nuance language within',\n",
       " 'technology accurately extract information insight contained document well categorize organize document']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.',\n",
       " 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv =CountVectorizer(binary=True, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  28  29  30  31  32  33  34  \\\n",
       "0   0   1   1   1   0   0   1   1   0   0  ...   1   1   1   1   1   1   0   \n",
       "1   0   0   0   0   1   0   1   0   0   1  ...   0   0   0   0   0   0   0   \n",
       "2   1   0   0   0   0   1   0   0   1   0  ...   0   0   0   0   0   0   1   \n",
       "\n",
       "   35  36  37  \n",
       "0   0   0   0  \n",
       "1   1   0   1  \n",
       "2   0   1   0  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv.fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 24, 'language': 21, 'processing': 30, 'nlp': 25, 'subfield': 33, 'linguistics': 23, 'computer': 6, 'science': 32, 'artificial': 3, 'intelligence': 19, 'concerned': 7, 'interaction': 20, 'human': 15, 'particular': 28, 'program': 31, 'process': 29, 'analyze': 2, 'large': 22, 'amount': 1, 'data': 11, 'goal': 14, 'capable': 4, 'understanding': 35, 'content': 9, 'document': 12, 'including': 16, 'contextual': 10, 'nuance': 26, 'within': 37, 'technology': 34, 'accurately': 0, 'extract': 13, 'information': 17, 'insight': 18, 'contained': 8, 'well': 36, 'categorize': 5, 'organize': 27}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.17842586, 0.17842586, 0.17842586, 0.        ,\n",
       "        0.        , 0.4070924 , 0.17842586, 0.        , 0.        ,\n",
       "        0.        , 0.17842586, 0.        , 0.        , 0.        ,\n",
       "        0.17842586, 0.        , 0.        , 0.        , 0.17842586,\n",
       "        0.17842586, 0.4070924 , 0.17842586, 0.17842586, 0.35685172,\n",
       "        0.17842586, 0.        , 0.        , 0.17842586, 0.17842586,\n",
       "        0.17842586, 0.17842586, 0.17842586, 0.17842586, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.32049968,\n",
       "        0.        , 0.24374827, 0.        , 0.        , 0.32049968,\n",
       "        0.32049968, 0.        , 0.24374827, 0.        , 0.32049968,\n",
       "        0.        , 0.32049968, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24374827, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32049968, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32049968, 0.        , 0.32049968],\n",
       "       [0.29730323, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.29730323, 0.        , 0.        , 0.29730323, 0.        ,\n",
       "        0.        , 0.        , 0.45221354, 0.29730323, 0.        ,\n",
       "        0.        , 0.        , 0.29730323, 0.29730323, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.29730323, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.29730323,\n",
       "        0.        , 0.29730323, 0.        ]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 24, 'language': 21, 'processing': 30, 'nlp': 25, 'subfield': 33, 'linguistics': 23, 'computer': 6, 'science': 32, 'artificial': 3, 'intelligence': 19, 'concerned': 7, 'interaction': 20, 'human': 15, 'particular': 28, 'program': 31, 'process': 29, 'analyze': 2, 'large': 22, 'amount': 1, 'data': 11, 'goal': 14, 'capable': 4, 'understanding': 35, 'content': 9, 'document': 12, 'including': 16, 'contextual': 10, 'nuance': 26, 'within': 37, 'technology': 34, 'accurately': 0, 'extract': 13, 'information': 17, 'insight': 18, 'contained': 8, 'well': 36, 'categorize': 5, 'organize': 27}\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "793c2e90bb751b9fa22bd2e5441ba81d9d7d4a4a528d5d69b923d56538a7c7bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
